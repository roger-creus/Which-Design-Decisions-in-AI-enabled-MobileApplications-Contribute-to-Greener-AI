{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "367d0ed4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-12-16T08:44:31.120068Z",
     "iopub.status.busy": "2021-12-16T08:44:31.118581Z",
     "iopub.status.idle": "2021-12-16T08:44:31.140477Z",
     "shell.execute_reply": "2021-12-16T08:44:31.141055Z",
     "shell.execute_reply.started": "2021-12-15T09:47:04.551436Z"
    },
    "papermill": {
     "duration": 0.043226,
     "end_time": "2021-12-16T08:44:31.141358",
     "exception": false,
     "start_time": "2021-12-16T08:44:31.098132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/amazonreviews/test.ft.txt.bz2\n",
      "/kaggle/input/amazonreviews/train.ft.txt.bz2\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7baebfac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T08:44:31.182049Z",
     "iopub.status.busy": "2021-12-16T08:44:31.181399Z",
     "iopub.status.idle": "2021-12-16T08:44:36.651580Z",
     "shell.execute_reply": "2021-12-16T08:44:36.650952Z",
     "shell.execute_reply.started": "2021-12-15T09:47:04.602525Z"
    },
    "papermill": {
     "duration": 5.491721,
     "end_time": "2021-12-16T08:44:36.651709",
     "exception": false,
     "start_time": "2021-12-16T08:44:31.159988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bz2\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfc70d7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T08:44:36.687847Z",
     "iopub.status.busy": "2021-12-16T08:44:36.687348Z",
     "iopub.status.idle": "2021-12-16T08:44:36.690715Z",
     "shell.execute_reply": "2021-12-16T08:44:36.690144Z",
     "shell.execute_reply.started": "2021-12-15T09:47:11.616003Z"
    },
    "papermill": {
     "duration": 0.022835,
     "end_time": "2021-12-16T08:44:36.690827",
     "exception": false,
     "start_time": "2021-12-16T08:44:36.667992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14eda361",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T08:44:36.727058Z",
     "iopub.status.busy": "2021-12-16T08:44:36.726341Z",
     "iopub.status.idle": "2021-12-16T08:44:36.728766Z",
     "shell.execute_reply": "2021-12-16T08:44:36.728367Z",
     "shell.execute_reply.started": "2021-12-15T09:47:11.628463Z"
    },
    "papermill": {
     "duration": 0.022482,
     "end_time": "2021-12-16T08:44:36.728868",
     "exception": false,
     "start_time": "2021-12-16T08:44:36.706386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def labels_texts(file):\n",
    "    labels = []\n",
    "    texts = []\n",
    "    for line in bz2.BZ2File(file):\n",
    "        x = line.decode(\"utf-8\")\n",
    "        labels.append(int(x[9]) - 1)\n",
    "        texts.append(x[10:].strip())\n",
    "    return np.array(labels), texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "123fab0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T08:44:36.763169Z",
     "iopub.status.busy": "2021-12-16T08:44:36.762687Z",
     "iopub.status.idle": "2021-12-16T08:46:36.945814Z",
     "shell.execute_reply": "2021-12-16T08:46:36.945279Z",
     "shell.execute_reply.started": "2021-12-15T09:47:11.649092Z"
    },
    "papermill": {
     "duration": 120.201702,
     "end_time": "2021-12-16T08:46:36.945963",
     "exception": false,
     "start_time": "2021-12-16T08:44:36.744261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_label, train_text = labels_texts('../input/amazonreviews/train.ft.txt.bz2')\n",
    "test_label, test_text = labels_texts('../input/amazonreviews/test.ft.txt.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f96b2097",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T08:46:36.982850Z",
     "iopub.status.busy": "2021-12-16T08:46:36.982116Z",
     "iopub.status.idle": "2021-12-16T08:46:36.985880Z",
     "shell.execute_reply": "2021-12-16T08:46:36.985350Z",
     "shell.execute_reply.started": "2021-12-15T09:49:33.606373Z"
    },
    "papermill": {
     "duration": 0.023563,
     "end_time": "2021-12-16T08:46:36.985995",
     "exception": false,
     "start_time": "2021-12-16T08:46:36.962432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\n"
     ]
    }
   ],
   "source": [
    "print(train_label[0])\n",
    "print(train_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41e97585",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T08:46:37.023684Z",
     "iopub.status.busy": "2021-12-16T08:46:37.022995Z",
     "iopub.status.idle": "2021-12-16T08:46:37.025066Z",
     "shell.execute_reply": "2021-12-16T08:46:37.025466Z",
     "shell.execute_reply.started": "2021-12-15T09:49:33.618974Z"
    },
    "papermill": {
     "duration": 0.023485,
     "end_time": "2021-12-16T08:46:37.025589",
     "exception": false,
     "start_time": "2021-12-16T08:46:37.002104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "not_numChar = re.compile(r'[\\W]')\n",
    "no_encode = re.compile(r'[^a-z0-1\\s]')\n",
    "def normalisation(texts):\n",
    "    norm_text = []\n",
    "    for word in texts:\n",
    "        lower = word.lower()\n",
    "        not_punct = not_numChar.sub(r' ', lower)\n",
    "        exclude_no_encode = no_encode.sub(r'', not_punct)\n",
    "        norm_text.append(exclude_no_encode)\n",
    "    return norm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0881fa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T08:46:37.085347Z",
     "iopub.status.busy": "2021-12-16T08:46:37.080220Z",
     "iopub.status.idle": "2021-12-16T08:49:34.391519Z",
     "shell.execute_reply": "2021-12-16T08:49:34.391008Z",
     "shell.execute_reply.started": "2021-12-15T09:49:33.631996Z"
    },
    "papermill": {
     "duration": 177.349916,
     "end_time": "2021-12-16T08:49:34.391662",
     "exception": false,
     "start_time": "2021-12-16T08:46:37.041746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_text = normalisation(train_text)\n",
    "test_text = normalisation(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a6b8293",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T08:49:34.430414Z",
     "iopub.status.busy": "2021-12-16T08:49:34.429786Z",
     "iopub.status.idle": "2021-12-16T08:49:34.432593Z",
     "shell.execute_reply": "2021-12-16T08:49:34.433164Z",
     "shell.execute_reply.started": "2021-12-15T09:52:56.562418Z"
    },
    "papermill": {
     "duration": 0.023264,
     "end_time": "2021-12-16T08:49:34.433345",
     "exception": false,
     "start_time": "2021-12-16T08:49:34.410081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stuning even for the non gamer  this sound track was beautiful  it paints the senery in your mind so well i would recomend it even to people who hate vid  game music  i have played the game chrono cross but out of all of the games i have ever played it has the best music  it backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras  it would impress anyone who cares to listen    \n"
     ]
    }
   ],
   "source": [
    "print(train_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fee94751",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T08:49:34.475283Z",
     "iopub.status.busy": "2021-12-16T08:49:34.473827Z",
     "iopub.status.idle": "2021-12-16T08:49:34.478748Z",
     "shell.execute_reply": "2021-12-16T08:49:34.479124Z",
     "shell.execute_reply.started": "2021-12-15T09:52:56.574224Z"
    },
    "papermill": {
     "duration": 0.028546,
     "end_time": "2021-12-16T08:49:34.479278",
     "exception": false,
     "start_time": "2021-12-16T08:49:34.450732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train = np.array(train_label)\n",
    "y_test = np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00117b6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T08:49:34.517532Z",
     "iopub.status.busy": "2021-12-16T08:49:34.516753Z",
     "iopub.status.idle": "2021-12-16T08:49:34.519736Z",
     "shell.execute_reply": "2021-12-16T08:49:34.520137Z",
     "shell.execute_reply.started": "2021-12-15T09:52:56.598686Z"
    },
    "papermill": {
     "duration": 0.024898,
     "end_time": "2021-12-16T08:49:34.520267",
     "exception": false,
     "start_time": "2021-12-16T08:49:34.495369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b18cae0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T08:49:34.580465Z",
     "iopub.status.busy": "2021-12-16T08:49:34.565105Z",
     "iopub.status.idle": "2021-12-16T08:53:24.713553Z",
     "shell.execute_reply": "2021-12-16T08:53:24.714001Z",
     "shell.execute_reply.started": "2021-12-15T09:52:56.818906Z"
    },
    "papermill": {
     "duration": 230.177511,
     "end_time": "2021-12-16T08:53:24.714168",
     "exception": false,
     "start_time": "2021-12-16T08:49:34.536657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size : 905946\n"
     ]
    }
   ],
   "source": [
    "max_features = 8192\n",
    "maxlen = 128\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocabulary Size :\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee0c5fab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T08:53:24.753286Z",
     "iopub.status.busy": "2021-12-16T08:53:24.752498Z",
     "iopub.status.idle": "2021-12-16T08:53:25.587104Z",
     "shell.execute_reply": "2021-12-16T08:53:25.586617Z",
     "shell.execute_reply.started": "2021-12-15T09:58:13.726016Z"
    },
    "papermill": {
     "duration": 0.8564,
     "end_time": "2021-12-16T08:53:25.587266",
     "exception": false,
     "start_time": "2021-12-16T08:53:24.730866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "with open('amazon_dictionary.txt', 'w') as file:\n",
    "    for key in word_index.keys():\n",
    "        file.write(key + \" \" + str(word_index[key]) + \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b366ed37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T08:53:25.654507Z",
     "iopub.status.busy": "2021-12-16T08:53:25.633887Z",
     "iopub.status.idle": "2021-12-16T08:56:52.730660Z",
     "shell.execute_reply": "2021-12-16T08:56:52.729543Z",
     "shell.execute_reply.started": "2021-12-15T09:58:14.779469Z"
    },
    "papermill": {
     "duration": 207.126357,
     "end_time": "2021-12-16T08:56:52.730843",
     "exception": false,
     "start_time": "2021-12-16T08:53:25.604486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_token = tokenizer.texts_to_sequences(train_text)\n",
    "testing_token = tokenizer.texts_to_sequences(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1abe152",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T08:56:52.800098Z",
     "iopub.status.busy": "2021-12-16T08:56:52.799291Z",
     "iopub.status.idle": "2021-12-16T08:58:07.229115Z",
     "shell.execute_reply": "2021-12-16T08:58:07.228606Z",
     "shell.execute_reply.started": "2021-12-15T10:02:34.793198Z"
    },
    "papermill": {
     "duration": 74.480384,
     "end_time": "2021-12-16T08:58:07.229281",
     "exception": false,
     "start_time": "2021-12-16T08:56:52.748897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train = pad_sequences(training_token, maxlen = maxlen, padding = 'post')\n",
    "x_test = pad_sequences(testing_token, maxlen = maxlen, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c74be41b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T08:58:07.270074Z",
     "iopub.status.busy": "2021-12-16T08:58:07.269474Z",
     "iopub.status.idle": "2021-12-16T08:58:08.892337Z",
     "shell.execute_reply": "2021-12-16T08:58:08.891748Z",
     "shell.execute_reply.started": "2021-12-15T10:03:54.502031Z"
    },
    "papermill": {
     "duration": 1.645763,
     "end_time": "2021-12-16T08:58:08.892482",
     "exception": false,
     "start_time": "2021-12-16T08:58:07.246719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\n",
    "test_data = TensorDataset(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE, drop_last = True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=BATCH_SIZE, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1e1160c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T08:58:08.941015Z",
     "iopub.status.busy": "2021-12-16T08:58:08.929430Z",
     "iopub.status.idle": "2021-12-16T08:58:08.944421Z",
     "shell.execute_reply": "2021-12-16T08:58:08.943947Z",
     "shell.execute_reply.started": "2021-12-15T10:19:14.696233Z"
    },
    "papermill": {
     "duration": 0.034523,
     "end_time": "2021-12-16T08:58:08.944539",
     "exception": false,
     "start_time": "2021-12-16T08:58:08.910016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.2):\n",
    "        super(SentimentNet, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "        self.encoder = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim, drop_prob)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(embedding_dim, 2, hidden_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        \n",
    "        self.fc = nn.Linear(embedding_dim * 128 , 1)\n",
    "\n",
    "    \n",
    "    def forward(self, src):\n",
    "        src = self.encoder(src) * math.sqrt(self.embedding_dim)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        \n",
    "        output = self.sigmoid(self.fc(output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47b4f437",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T08:58:08.984237Z",
     "iopub.status.busy": "2021-12-16T08:58:08.982694Z",
     "iopub.status.idle": "2021-12-16T08:58:08.984853Z",
     "shell.execute_reply": "2021-12-16T08:58:08.985279Z",
     "shell.execute_reply.started": "2021-12-15T10:04:00.284359Z"
    },
    "papermill": {
     "duration": 0.023626,
     "end_time": "2021-12-16T08:58:08.985404",
     "exception": false,
     "start_time": "2021-12-16T08:58:08.961778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2aa9c8c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T08:58:09.025870Z",
     "iopub.status.busy": "2021-12-16T08:58:09.025361Z",
     "iopub.status.idle": "2021-12-16T08:58:14.793890Z",
     "shell.execute_reply": "2021-12-16T08:58:14.794573Z",
     "shell.execute_reply.started": "2021-12-15T10:19:04.882323Z"
    },
    "papermill": {
     "duration": 5.79228,
     "end_time": "2021-12-16T08:58:14.794735",
     "exception": false,
     "start_time": "2021-12-16T08:58:09.002455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116045189\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "output_size = 1\n",
    "embedding_dim = 128\n",
    "hidden_dim = 4\n",
    "n_layers = 1\n",
    "\n",
    "model = SentimentNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "model.to(device)\n",
    "print(model_params(model))\n",
    "lr=0.01\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e5c2e6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T08:58:14.840296Z",
     "iopub.status.busy": "2021-12-16T08:58:14.839748Z",
     "iopub.status.idle": "2021-12-16T10:01:57.513436Z",
     "shell.execute_reply": "2021-12-16T10:01:57.513856Z",
     "shell.execute_reply.started": "2021-12-15T10:19:07.623429Z"
    },
    "papermill": {
     "duration": 3822.701733,
     "end_time": "2021-12-16T10:01:57.514084",
     "exception": false,
     "start_time": "2021-12-16T08:58:14.812351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1... Step: 500... Loss: 44.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (inf --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 1000... Loss: 50.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 1500... Loss: 56.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 2000... Loss: 40.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 2500... Loss: 52.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 3000... Loss: 44.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 3500... Loss: 48.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 4000... Loss: 46.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 4500... Loss: 46.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 5000... Loss: 50.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 5500... Loss: 48.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 6000... Loss: 56.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 6500... Loss: 50.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 7000... Loss: 42.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 7500... Loss: 56.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 8000... Loss: 58.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 8500... Loss: 50.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 9000... Loss: 54.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 9500... Loss: 36.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 10000... Loss: 60.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 10500... Loss: 48.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 11000... Loss: 52.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 11500... Loss: 48.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 12000... Loss: 44.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 12500... Loss: 48.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 13000... Loss: 60.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 13500... Loss: 42.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 14000... Loss: 64.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 14500... Loss: 52.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 15000... Loss: 62.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 15500... Loss: 52.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 16000... Loss: 42.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 16500... Loss: 62.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 17000... Loss: 50.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 17500... Loss: 48.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 18000... Loss: 58.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 18500... Loss: 52.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 19000... Loss: 40.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 19500... Loss: 66.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 20000... Loss: 48.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 20500... Loss: 68.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 21000... Loss: 40.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 21500... Loss: 58.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 22000... Loss: 42.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 22500... Loss: 56.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 23000... Loss: 62.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 23500... Loss: 56.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 24000... Loss: 38.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 24500... Loss: 60.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 25000... Loss: 48.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 25500... Loss: 48.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 26000... Loss: 48.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 26500... Loss: 58.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 27000... Loss: 50.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 27500... Loss: 52.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 28000... Loss: 54.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 28500... Loss: 42.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 29000... Loss: 48.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 29500... Loss: 44.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 30000... Loss: 52.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 30500... Loss: 54.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 31000... Loss: 46.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 31500... Loss: 44.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 32000... Loss: 50.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 32500... Loss: 52.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 33000... Loss: 48.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 33500... Loss: 66.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 34000... Loss: 48.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 34500... Loss: 38.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 35000... Loss: 42.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 35500... Loss: 46.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 36000... Loss: 44.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 36500... Loss: 38.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 37000... Loss: 44.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 37500... Loss: 48.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 38000... Loss: 56.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 38500... Loss: 46.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 39000... Loss: 28.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 39500... Loss: 56.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 40000... Loss: 54.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 40500... Loss: 54.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 41000... Loss: 54.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 41500... Loss: 56.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 42000... Loss: 64.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 42500... Loss: 48.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 43000... Loss: 54.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 43500... Loss: 50.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 44000... Loss: 58.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 44500... Loss: 54.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 45000... Loss: 46.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 45500... Loss: 42.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 46000... Loss: 34.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 46500... Loss: 50.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 47000... Loss: 40.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 47500... Loss: 44.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 48000... Loss: 60.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 48500... Loss: 40.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 49000... Loss: 60.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 49500... Loss: 48.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 50000... Loss: 54.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 50500... Loss: 52.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 51000... Loss: 56.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 51500... Loss: 44.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 52000... Loss: 38.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 52500... Loss: 42.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 53000... Loss: 60.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 53500... Loss: 48.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 54000... Loss: 54.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 54500... Loss: 56.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 55000... Loss: 48.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 55500... Loss: 62.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 56000... Loss: 42.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 56500... Loss: 54.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 57000... Loss: 42.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 57500... Loss: 58.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 58000... Loss: 52.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 58500... Loss: 44.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 59000... Loss: 52.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 59500... Loss: 42.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 60000... Loss: 44.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 60500... Loss: 50.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 61000... Loss: 50.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 61500... Loss: 48.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 62000... Loss: 52.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 62500... Loss: 54.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 63000... Loss: 46.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 63500... Loss: 50.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 64000... Loss: 64.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 64500... Loss: 66.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 65000... Loss: 54.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 65500... Loss: 52.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 66000... Loss: 46.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 66500... Loss: 44.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 67000... Loss: 46.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 67500... Loss: 48.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 68000... Loss: 56.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 68500... Loss: 52.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 69000... Loss: 44.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 69500... Loss: 50.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 70000... Loss: 50.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 70500... Loss: 46.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 71000... Loss: 50.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 71500... Loss: 52.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n",
      "Epoch: 1/1... Step: 72000... Loss: 50.000000... Val Loss: 50.000000\n",
      "Validation loss decreased (50.000000 --> 50.000000).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "counter = 0\n",
    "print_every = 500\n",
    "clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output = model(inputs.long())\n",
    "        \n",
    "        loss = criterion(output.squeeze(1), labels.float())\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter%print_every == 0:\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in test_loader:\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out = model(inp.long())\n",
    "                val_loss = criterion(out.squeeze(1), lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './state_dict.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed77ab65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T10:01:57.706765Z",
     "iopub.status.busy": "2021-12-16T10:01:57.703706Z",
     "iopub.status.idle": "2021-12-16T10:01:57.756637Z",
     "shell.execute_reply": "2021-12-16T10:01:57.757004Z",
     "shell.execute_reply.started": "2021-12-15T10:04:08.106311Z"
    },
    "papermill": {
     "duration": 0.153826,
     "end_time": "2021-12-16T10:01:57.757143",
     "exception": false,
     "start_time": "2021-12-16T10:01:57.603317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.]], device='cuda:0', grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I love you\"\n",
    "trial = torch.tensor(pad_sequences(tokenizer.texts_to_sequences([sentence]), maxlen = maxlen)).long().to(device)\n",
    "\n",
    "model(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a74fb2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T10:01:57.941199Z",
     "iopub.status.busy": "2021-12-16T10:01:57.940045Z",
     "iopub.status.idle": "2021-12-16T10:02:05.436870Z",
     "shell.execute_reply": "2021-12-16T10:02:05.436394Z",
     "shell.execute_reply.started": "2021-12-15T10:04:08.108493Z"
    },
    "papermill": {
     "duration": 7.590034,
     "end_time": "2021-12-16T10:02:05.437002",
     "exception": false,
     "start_time": "2021-12-16T10:01:57.846968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "torch.onnx.export(model,               # model being run\n",
    "                  trial,                  # model input (or a tuple for multiple inputs)\n",
    "                  \"transformer-amazon.onnx\", opset_version = 11)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4665.53296,
   "end_time": "2021-12-16T10:02:08.470044",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-12-16T08:44:22.937084",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
