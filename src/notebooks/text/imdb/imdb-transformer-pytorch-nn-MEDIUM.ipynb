{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e3ca1a2",
   "metadata": {
    "papermill": {
     "duration": 0.015674,
     "end_time": "2021-11-26T12:04:08.199486",
     "exception": false,
     "start_time": "2021-11-26T12:04:08.183812",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Sentiment Analysis using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3aca89e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-26T12:04:08.307447Z",
     "iopub.status.busy": "2021-11-26T12:04:08.306576Z",
     "iopub.status.idle": "2021-11-26T12:04:17.940330Z",
     "shell.execute_reply": "2021-11-26T12:04:17.939730Z",
     "shell.execute_reply.started": "2021-11-26T11:46:05.391441Z"
    },
    "papermill": {
     "duration": 9.726266,
     "end_time": "2021-11-26T12:04:17.940479",
     "exception": false,
     "start_time": "2021-11-26T12:04:08.214213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-26 12:04:14.458075: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bz2\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdd0bcb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-26T12:04:17.978104Z",
     "iopub.status.busy": "2021-11-26T12:04:17.976521Z",
     "iopub.status.idle": "2021-11-26T12:04:17.978708Z",
     "shell.execute_reply": "2021-11-26T12:04:17.979115Z",
     "shell.execute_reply.started": "2021-11-26T11:46:05.409146Z"
    },
    "papermill": {
     "duration": 0.021149,
     "end_time": "2021-11-26T12:04:17.979241",
     "exception": false,
     "start_time": "2021-11-26T12:04:17.958092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7399ed21",
   "metadata": {
    "papermill": {
     "duration": 0.015243,
     "end_time": "2021-11-26T12:04:18.009455",
     "exception": false,
     "start_time": "2021-11-26T12:04:17.994212",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1) Load in and visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46e7d8f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-26T12:04:18.043238Z",
     "iopub.status.busy": "2021-11-26T12:04:18.042739Z",
     "iopub.status.idle": "2021-11-26T12:04:19.705435Z",
     "shell.execute_reply": "2021-11-26T12:04:19.705930Z",
     "shell.execute_reply.started": "2021-11-26T11:46:05.422503Z"
    },
    "papermill": {
     "duration": 1.681845,
     "end_time": "2021-11-26T12:04:19.706108",
     "exception": false,
     "start_time": "2021-11-26T12:04:18.024263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8d9b01",
   "metadata": {
    "papermill": {
     "duration": 0.014958,
     "end_time": "2021-11-26T12:04:19.737935",
     "exception": false,
     "start_time": "2021-11-26T12:04:19.722977",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2) Data Processing â€” convert to lower case, Remove punctuation etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f7e1a90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-26T12:04:19.789943Z",
     "iopub.status.busy": "2021-11-26T12:04:19.784862Z",
     "iopub.status.idle": "2021-11-26T12:04:29.398348Z",
     "shell.execute_reply": "2021-11-26T12:04:29.398764Z",
     "shell.execute_reply.started": "2021-11-26T11:46:05.957734Z"
    },
    "papermill": {
     "duration": 9.645598,
     "end_time": "2021-11-26T12:04:29.398924",
     "exception": false,
     "start_time": "2021-11-26T12:04:19.753326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>one reviewers mentioned watching 1 oz episode ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>basically theres family little boy jake thinks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>petter matteis love time money visually stunni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                     cleaned_reviews  \n",
       "0  one reviewers mentioned watching 1 oz episode ...  \n",
       "1  wonderful little production filming technique ...  \n",
       "2  thought wonderful way spend time hot summer we...  \n",
       "3  basically theres family little boy jake thinks...  \n",
       "4  petter matteis love time money visually stunni...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data_preprocessing(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('<.*?>', '', text) # Remove HTML from text\n",
    "    text = ''.join([c for c in text if c not in string.punctuation])# Remove punctuation\n",
    "    text = [word for word in text.split() if word not in stop_words]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "df['cleaned_reviews'] = df['review'].apply(data_preprocessing)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3975aa86",
   "metadata": {
    "papermill": {
     "duration": 0.015501,
     "end_time": "2021-11-26T12:04:29.430222",
     "exception": false,
     "start_time": "2021-11-26T12:04:29.414721",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5) Tokenize â€” Create Vocab to Int mapping dictionary\n",
    "In most of the NLP tasks, you will create an index mapping dictionary in such a way that your frequently occurring words are assigned lower indexes. One of the most common way of doing this is to use Counter method from Collections library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcbb27e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-26T12:04:29.520213Z",
     "iopub.status.busy": "2021-11-26T12:04:29.484623Z",
     "iopub.status.idle": "2021-11-26T12:04:35.590700Z",
     "shell.execute_reply": "2021-11-26T12:04:35.591682Z",
     "shell.execute_reply.started": "2021-11-26T11:46:15.690609Z"
    },
    "papermill": {
     "duration": 6.145749,
     "end_time": "2021-11-26T12:04:35.591885",
     "exception": false,
     "start_time": "2021-11-26T12:04:29.446136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size : 222610\n"
     ]
    }
   ],
   "source": [
    "max_features = 8192\n",
    "maxlen = 30\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(df['cleaned_reviews'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocabulary Size :\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aee0ab73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-26T12:04:35.655166Z",
     "iopub.status.busy": "2021-11-26T12:04:35.654363Z",
     "iopub.status.idle": "2021-11-26T12:04:40.247951Z",
     "shell.execute_reply": "2021-11-26T12:04:40.247410Z",
     "shell.execute_reply.started": "2021-11-26T11:46:21.828166Z"
    },
    "papermill": {
     "duration": 4.62923,
     "end_time": "2021-11-26T12:04:40.248092",
     "exception": false,
     "start_time": "2021-11-26T12:04:35.618862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_token = tokenizer.texts_to_sequences(df['cleaned_reviews'])\n",
    "x_data = pad_sequences(training_token, maxlen = maxlen, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39e2fb86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-26T12:04:40.284836Z",
     "iopub.status.busy": "2021-11-26T12:04:40.284078Z",
     "iopub.status.idle": "2021-11-26T12:04:40.314749Z",
     "shell.execute_reply": "2021-11-26T12:04:40.314325Z",
     "shell.execute_reply.started": "2021-11-26T11:46:26.370519Z"
    },
    "papermill": {
     "duration": 0.050536,
     "end_time": "2021-11-26T12:04:40.314859",
     "exception": false,
     "start_time": "2021-11-26T12:04:40.264323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_data = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55682a47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-26T12:04:40.350791Z",
     "iopub.status.busy": "2021-11-26T12:04:40.350223Z",
     "iopub.status.idle": "2021-11-26T12:04:40.363105Z",
     "shell.execute_reply": "2021-11-26T12:04:40.362717Z",
     "shell.execute_reply.started": "2021-11-26T11:46:26.405302Z"
    },
    "papermill": {
     "duration": 0.032733,
     "end_time": "2021-11-26T12:04:40.363206",
     "exception": false,
     "start_time": "2021-11-26T12:04:40.330473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_remain, y_train, y_remain = train_test_split(x_data, y_data, test_size=0.2, random_state=1)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_remain, y_remain, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5d597dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-26T12:04:40.398397Z",
     "iopub.status.busy": "2021-11-26T12:04:40.397668Z",
     "iopub.status.idle": "2021-11-26T12:04:40.400057Z",
     "shell.execute_reply": "2021-11-26T12:04:40.399639Z",
     "shell.execute_reply.started": "2021-11-26T11:46:26.422158Z"
    },
    "papermill": {
     "duration": 0.02138,
     "end_time": "2021-11-26T12:04:40.400154",
     "exception": false,
     "start_time": "2021-11-26T12:04:40.378774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train = y_train.to_numpy().reshape(-1,1)\n",
    "y_test = y_test.to_numpy().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52b1aef9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-26T12:04:40.438393Z",
     "iopub.status.busy": "2021-11-26T12:04:40.437253Z",
     "iopub.status.idle": "2021-11-26T12:04:40.455093Z",
     "shell.execute_reply": "2021-11-26T12:04:40.454652Z",
     "shell.execute_reply.started": "2021-11-26T11:46:26.428343Z"
    },
    "papermill": {
     "duration": 0.039438,
     "end_time": "2021-11-26T12:04:40.455200",
     "exception": false,
     "start_time": "2021-11-26T12:04:40.415762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create tensor dataset\n",
    "train_data = TensorDataset(torch.from_numpy(X_train.astype('float64')), torch.from_numpy(np.array(y_train).astype('float64')))\n",
    "test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(np.array(y_test).astype('float64')))\n",
    "valid_data = TensorDataset(torch.from_numpy(X_valid), torch.from_numpy(np.array(y_valid).astype('float64')))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90446b8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-26T12:04:40.493350Z",
     "iopub.status.busy": "2021-11-26T12:04:40.492772Z",
     "iopub.status.idle": "2021-11-26T12:04:40.559892Z",
     "shell.execute_reply": "2021-11-26T12:04:40.560336Z",
     "shell.execute_reply.started": "2021-11-26T11:46:26.444992Z"
    },
    "papermill": {
     "duration": 0.08943,
     "end_time": "2021-11-26T12:04:40.560470",
     "exception": false,
     "start_time": "2021-11-26T12:04:40.471040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 30])\n",
      "Sample input: \n",
      " tensor([[3.6000e+01, 5.7400e+02, 1.6000e+01,  ..., 7.5700e+02, 4.8300e+02,\n",
      "         7.4700e+02],\n",
      "        [4.5050e+03, 3.2140e+03, 2.0280e+03,  ..., 1.5400e+02, 5.0480e+03,\n",
      "         2.7000e+01],\n",
      "        [2.6500e+02, 4.8420e+03, 2.1610e+03,  ..., 1.4000e+02, 7.3100e+02,\n",
      "         1.2600e+02],\n",
      "        ...,\n",
      "        [6.0000e+01, 3.9000e+01, 1.7560e+03,  ..., 5.4600e+02, 3.2500e+02,\n",
      "         8.0000e+00],\n",
      "        [2.2000e+01, 1.4040e+03, 1.0000e+00,  ..., 1.3500e+02, 3.5000e+01,\n",
      "         3.0000e+00],\n",
      "        [1.2400e+03, 4.0000e+01, 1.8000e+01,  ..., 2.3100e+02, 4.5200e+02,\n",
      "         4.2000e+01]], dtype=torch.float64)\n",
      "Sample input: \n",
      " tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print('Sample input: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69756155",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-26T12:04:40.601940Z",
     "iopub.status.busy": "2021-11-26T12:04:40.601214Z",
     "iopub.status.idle": "2021-11-26T12:04:40.603428Z",
     "shell.execute_reply": "2021-11-26T12:04:40.603829Z",
     "shell.execute_reply.started": "2021-11-26T11:46:26.462967Z"
    },
    "papermill": {
     "duration": 0.02681,
     "end_time": "2021-11-26T12:04:40.603953",
     "exception": false,
     "start_time": "2021-11-26T12:04:40.577143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5db38b52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-26T12:04:40.645940Z",
     "iopub.status.busy": "2021-11-26T12:04:40.645093Z",
     "iopub.status.idle": "2021-11-26T12:04:40.649607Z",
     "shell.execute_reply": "2021-11-26T12:04:40.649151Z",
     "shell.execute_reply.started": "2021-11-26T11:55:38.852669Z"
    },
    "papermill": {
     "duration": 0.028616,
     "end_time": "2021-11-26T12:04:40.649721",
     "exception": false,
     "start_time": "2021-11-26T12:04:40.621105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.2):\n",
    "        super(SentimentNet, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "        self.encoder = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim, drop_prob)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(embedding_dim, 2, hidden_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "\n",
    "        self.fc = nn.Linear(embedding_dim * 30 , 1)\n",
    "\n",
    "     \n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        if len(x) != 1:\n",
    "            batch_size = x.size(0)\n",
    "\n",
    "        x = x.long()\n",
    "        \n",
    "        embeds = self.embedding(x)\n",
    "        \n",
    "        print(embeds.shape)\n",
    "        \n",
    "        out = self.transformer_encoder(embeds)\n",
    "        \n",
    "        print(out.shape)\n",
    "\n",
    "        out = self.fc(out)\n",
    "                \n",
    "        print(out.shape)\n",
    "\n",
    "        return out\n",
    "        \"\"\"\n",
    "    \n",
    "    def forward(self, src):\n",
    "        src = self.encoder(src) * math.sqrt(self.embedding_dim)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.sigmoid(self.fc(output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7181d07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-26T12:04:40.687243Z",
     "iopub.status.busy": "2021-11-26T12:04:40.686530Z",
     "iopub.status.idle": "2021-11-26T12:04:40.688797Z",
     "shell.execute_reply": "2021-11-26T12:04:40.689181Z",
     "shell.execute_reply.started": "2021-11-26T11:46:26.486603Z"
    },
    "papermill": {
     "duration": 0.023125,
     "end_time": "2021-11-26T12:04:40.689305",
     "exception": false,
     "start_time": "2021-11-26T12:04:40.666180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d62ff45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-26T12:04:40.727554Z",
     "iopub.status.busy": "2021-11-26T12:04:40.726987Z",
     "iopub.status.idle": "2021-11-26T12:04:47.661980Z",
     "shell.execute_reply": "2021-11-26T12:04:47.662362Z",
     "shell.execute_reply.started": "2021-11-26T11:57:54.079151Z"
    },
    "papermill": {
     "duration": 6.956515,
     "end_time": "2021-11-26T12:04:47.662539",
     "exception": false,
     "start_time": "2021-11-26T12:04:40.706024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2229705\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "output_size = 1\n",
    "embedding_dim = 10\n",
    "hidden_dim = 16\n",
    "n_layers = 4\n",
    "\n",
    "model = SentimentNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "model.to(device)\n",
    "print(model_params(model))\n",
    "lr=0.008\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53688946",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-26T12:04:47.706392Z",
     "iopub.status.busy": "2021-11-26T12:04:47.702593Z",
     "iopub.status.idle": "2021-11-26T12:07:41.754820Z",
     "shell.execute_reply": "2021-11-26T12:07:41.754360Z",
     "shell.execute_reply.started": "2021-11-26T11:55:49.820496Z"
    },
    "papermill": {
     "duration": 174.07593,
     "end_time": "2021-11-26T12:07:41.754957",
     "exception": false,
     "start_time": "2021-11-26T12:04:47.679027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Step: 100... Loss: 0.708416... Val Loss: 0.693570\n",
      "Validation loss decreased (inf --> 0.693570).  Saving model ...\n",
      "Epoch: 1/10... Step: 200... Loss: 0.696573... Val Loss: 0.702668\n",
      "Epoch: 1/10... Step: 300... Loss: 0.687900... Val Loss: 0.693139\n",
      "Validation loss decreased (0.693570 --> 0.693139).  Saving model ...\n",
      "Epoch: 1/10... Step: 400... Loss: 0.686965... Val Loss: 0.692842\n",
      "Validation loss decreased (0.693139 --> 0.692842).  Saving model ...\n",
      "Epoch: 1/10... Step: 500... Loss: 0.691707... Val Loss: 0.692796\n",
      "Validation loss decreased (0.692842 --> 0.692796).  Saving model ...\n",
      "Epoch: 1/10... Step: 600... Loss: 0.695244... Val Loss: 0.693483\n",
      "Epoch: 1/10... Step: 700... Loss: 0.691158... Val Loss: 0.692591\n",
      "Validation loss decreased (0.692796 --> 0.692591).  Saving model ...\n",
      "Epoch: 1/10... Step: 800... Loss: 0.687895... Val Loss: 0.693097\n",
      "Epoch: 2/10... Step: 900... Loss: 0.697914... Val Loss: 0.693055\n",
      "Epoch: 2/10... Step: 1000... Loss: 0.685920... Val Loss: 0.691086\n",
      "Validation loss decreased (0.692591 --> 0.691086).  Saving model ...\n",
      "Epoch: 2/10... Step: 1100... Loss: 0.680635... Val Loss: 0.685051\n",
      "Validation loss decreased (0.691086 --> 0.685051).  Saving model ...\n",
      "Epoch: 2/10... Step: 1200... Loss: 0.675073... Val Loss: 0.695182\n",
      "Epoch: 2/10... Step: 1300... Loss: 0.683285... Val Loss: 0.681767\n",
      "Validation loss decreased (0.685051 --> 0.681767).  Saving model ...\n",
      "Epoch: 2/10... Step: 1400... Loss: 0.668217... Val Loss: 0.681359\n",
      "Validation loss decreased (0.681767 --> 0.681359).  Saving model ...\n",
      "Epoch: 2/10... Step: 1500... Loss: 0.658390... Val Loss: 0.679889\n",
      "Validation loss decreased (0.681359 --> 0.679889).  Saving model ...\n",
      "Epoch: 2/10... Step: 1600... Loss: 0.732452... Val Loss: 0.679763\n",
      "Validation loss decreased (0.679889 --> 0.679763).  Saving model ...\n",
      "Epoch: 3/10... Step: 1700... Loss: 0.624133... Val Loss: 0.676521\n",
      "Validation loss decreased (0.679763 --> 0.676521).  Saving model ...\n",
      "Epoch: 3/10... Step: 1800... Loss: 0.662796... Val Loss: 0.668848\n",
      "Validation loss decreased (0.676521 --> 0.668848).  Saving model ...\n",
      "Epoch: 3/10... Step: 1900... Loss: 0.634749... Val Loss: 0.665786\n",
      "Validation loss decreased (0.668848 --> 0.665786).  Saving model ...\n",
      "Epoch: 3/10... Step: 2000... Loss: 0.705292... Val Loss: 0.664876\n",
      "Validation loss decreased (0.665786 --> 0.664876).  Saving model ...\n",
      "Epoch: 3/10... Step: 2100... Loss: 0.597230... Val Loss: 0.657714\n",
      "Validation loss decreased (0.664876 --> 0.657714).  Saving model ...\n",
      "Epoch: 3/10... Step: 2200... Loss: 0.643154... Val Loss: 0.652242\n",
      "Validation loss decreased (0.657714 --> 0.652242).  Saving model ...\n",
      "Epoch: 3/10... Step: 2300... Loss: 0.635577... Val Loss: 0.645262\n",
      "Validation loss decreased (0.652242 --> 0.645262).  Saving model ...\n",
      "Epoch: 3/10... Step: 2400... Loss: 0.627703... Val Loss: 0.638489\n",
      "Validation loss decreased (0.645262 --> 0.638489).  Saving model ...\n",
      "Epoch: 4/10... Step: 2500... Loss: 0.772840... Val Loss: 0.620444\n",
      "Validation loss decreased (0.638489 --> 0.620444).  Saving model ...\n",
      "Epoch: 4/10... Step: 2600... Loss: 0.642095... Val Loss: 0.611227\n",
      "Validation loss decreased (0.620444 --> 0.611227).  Saving model ...\n",
      "Epoch: 4/10... Step: 2700... Loss: 0.559747... Val Loss: 0.594229\n",
      "Validation loss decreased (0.611227 --> 0.594229).  Saving model ...\n",
      "Epoch: 4/10... Step: 2800... Loss: 0.618889... Val Loss: 0.563052\n",
      "Validation loss decreased (0.594229 --> 0.563052).  Saving model ...\n",
      "Epoch: 4/10... Step: 2900... Loss: 0.571557... Val Loss: 0.544229\n",
      "Validation loss decreased (0.563052 --> 0.544229).  Saving model ...\n",
      "Epoch: 4/10... Step: 3000... Loss: 0.660815... Val Loss: 0.519467\n",
      "Validation loss decreased (0.544229 --> 0.519467).  Saving model ...\n",
      "Epoch: 4/10... Step: 3100... Loss: 0.607506... Val Loss: 0.552894\n",
      "Epoch: 4/10... Step: 3200... Loss: 0.703332... Val Loss: 0.497126\n",
      "Validation loss decreased (0.519467 --> 0.497126).  Saving model ...\n",
      "Epoch: 5/10... Step: 3300... Loss: 0.516164... Val Loss: 0.481847\n",
      "Validation loss decreased (0.497126 --> 0.481847).  Saving model ...\n",
      "Epoch: 5/10... Step: 3400... Loss: 0.505626... Val Loss: 0.469792\n",
      "Validation loss decreased (0.481847 --> 0.469792).  Saving model ...\n",
      "Epoch: 5/10... Step: 3500... Loss: 0.472966... Val Loss: 0.469058\n",
      "Validation loss decreased (0.469792 --> 0.469058).  Saving model ...\n",
      "Epoch: 5/10... Step: 3600... Loss: 0.486210... Val Loss: 0.465288\n",
      "Validation loss decreased (0.469058 --> 0.465288).  Saving model ...\n",
      "Epoch: 5/10... Step: 3700... Loss: 0.396019... Val Loss: 0.458851\n",
      "Validation loss decreased (0.465288 --> 0.458851).  Saving model ...\n",
      "Epoch: 5/10... Step: 3800... Loss: 0.508281... Val Loss: 0.441244\n",
      "Validation loss decreased (0.458851 --> 0.441244).  Saving model ...\n",
      "Epoch: 5/10... Step: 3900... Loss: 0.486391... Val Loss: 0.439544\n",
      "Validation loss decreased (0.441244 --> 0.439544).  Saving model ...\n",
      "Epoch: 5/10... Step: 4000... Loss: 0.357058... Val Loss: 0.427206\n",
      "Validation loss decreased (0.439544 --> 0.427206).  Saving model ...\n",
      "Epoch: 6/10... Step: 4100... Loss: 0.454936... Val Loss: 0.436480\n",
      "Epoch: 6/10... Step: 4200... Loss: 0.393410... Val Loss: 0.431099\n",
      "Epoch: 6/10... Step: 4300... Loss: 0.403120... Val Loss: 0.411405\n",
      "Validation loss decreased (0.427206 --> 0.411405).  Saving model ...\n",
      "Epoch: 6/10... Step: 4400... Loss: 0.362076... Val Loss: 0.435657\n",
      "Epoch: 6/10... Step: 4500... Loss: 0.384686... Val Loss: 0.440946\n",
      "Epoch: 6/10... Step: 4600... Loss: 0.376759... Val Loss: 0.419768\n",
      "Epoch: 6/10... Step: 4700... Loss: 0.468274... Val Loss: 0.415814\n",
      "Epoch: 6/10... Step: 4800... Loss: 0.400189... Val Loss: 0.412127\n",
      "Epoch: 7/10... Step: 4900... Loss: 0.441287... Val Loss: 0.428543\n",
      "Epoch: 7/10... Step: 5000... Loss: 0.327069... Val Loss: 0.405710\n",
      "Validation loss decreased (0.411405 --> 0.405710).  Saving model ...\n",
      "Epoch: 7/10... Step: 5100... Loss: 0.387834... Val Loss: 0.416307\n",
      "Epoch: 7/10... Step: 5200... Loss: 0.384179... Val Loss: 0.436189\n",
      "Epoch: 7/10... Step: 5300... Loss: 0.462392... Val Loss: 0.399858\n",
      "Validation loss decreased (0.405710 --> 0.399858).  Saving model ...\n",
      "Epoch: 7/10... Step: 5400... Loss: 0.381946... Val Loss: 0.406753\n",
      "Epoch: 7/10... Step: 5500... Loss: 0.208424... Val Loss: 0.405558\n",
      "Epoch: 7/10... Step: 5600... Loss: 0.246417... Val Loss: 0.403434\n",
      "Epoch: 8/10... Step: 5700... Loss: 0.440037... Val Loss: 0.425375\n",
      "Epoch: 8/10... Step: 5800... Loss: 0.335009... Val Loss: 0.401432\n",
      "Epoch: 8/10... Step: 5900... Loss: 0.318635... Val Loss: 0.400575\n",
      "Epoch: 8/10... Step: 6000... Loss: 0.344766... Val Loss: 0.391764\n",
      "Validation loss decreased (0.399858 --> 0.391764).  Saving model ...\n",
      "Epoch: 8/10... Step: 6100... Loss: 0.351204... Val Loss: 0.407641\n",
      "Epoch: 8/10... Step: 6200... Loss: 0.263863... Val Loss: 0.399657\n",
      "Epoch: 8/10... Step: 6300... Loss: 0.587247... Val Loss: 0.413427\n",
      "Epoch: 8/10... Step: 6400... Loss: 0.310689... Val Loss: 0.410672\n",
      "Epoch: 9/10... Step: 6500... Loss: 0.480535... Val Loss: 0.416664\n",
      "Epoch: 9/10... Step: 6600... Loss: 0.504864... Val Loss: 0.396206\n",
      "Epoch: 9/10... Step: 6700... Loss: 0.456410... Val Loss: 0.403151\n",
      "Epoch: 9/10... Step: 6800... Loss: 0.399259... Val Loss: 0.387120\n",
      "Validation loss decreased (0.391764 --> 0.387120).  Saving model ...\n",
      "Epoch: 9/10... Step: 6900... Loss: 0.235009... Val Loss: 0.427520\n",
      "Epoch: 9/10... Step: 7000... Loss: 0.391518... Val Loss: 0.403639\n",
      "Epoch: 9/10... Step: 7100... Loss: 0.487010... Val Loss: 0.404431\n",
      "Epoch: 9/10... Step: 7200... Loss: 0.396379... Val Loss: 0.397517\n",
      "Epoch: 10/10... Step: 7300... Loss: 0.429430... Val Loss: 0.408029\n",
      "Epoch: 10/10... Step: 7400... Loss: 0.414444... Val Loss: 0.403592\n",
      "Epoch: 10/10... Step: 7500... Loss: 0.327338... Val Loss: 0.394254\n",
      "Epoch: 10/10... Step: 7600... Loss: 0.387701... Val Loss: 0.405216\n",
      "Epoch: 10/10... Step: 7700... Loss: 0.292894... Val Loss: 0.393959\n",
      "Epoch: 10/10... Step: 7800... Loss: 0.271554... Val Loss: 0.394164\n",
      "Epoch: 10/10... Step: 7900... Loss: 0.369581... Val Loss: 0.383916\n",
      "Validation loss decreased (0.387120 --> 0.383916).  Saving model ...\n",
      "Epoch: 10/10... Step: 8000... Loss: 0.339575... Val Loss: 0.396531\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output = model(inputs.long())\n",
    "        \n",
    "        loss = criterion(output, labels.float())\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter%print_every == 0:\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in test_loader:\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out = model(inp.long())\n",
    "                val_loss = criterion(out, lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './state_dict.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b8873fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-26T12:07:41.834849Z",
     "iopub.status.busy": "2021-11-26T12:07:41.834009Z",
     "iopub.status.idle": "2021-11-26T12:07:41.860520Z",
     "shell.execute_reply": "2021-11-26T12:07:41.860935Z",
     "shell.execute_reply.started": "2021-11-26T11:54:27.172539Z"
    },
    "papermill": {
     "duration": 0.068669,
     "end_time": "2021-11-26T12:07:41.861057",
     "exception": false,
     "start_time": "2021-11-26T12:07:41.792388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8192]], device='cuda:0', grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I love you\"\n",
    "trial = torch.tensor(pad_sequences(tokenizer.texts_to_sequences([sentence]), maxlen = maxlen)).long().to(device)\n",
    "\n",
    "model(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a47850f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-26T12:07:41.939001Z",
     "iopub.status.busy": "2021-11-26T12:07:41.938450Z",
     "iopub.status.idle": "2021-11-26T12:07:42.546396Z",
     "shell.execute_reply": "2021-11-26T12:07:42.545913Z",
     "shell.execute_reply.started": "2021-11-26T11:54:27.174417Z"
    },
    "papermill": {
     "duration": 0.648994,
     "end_time": "2021-11-26T12:07:42.546561",
     "exception": false,
     "start_time": "2021-11-26T12:07:41.897567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:16: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "import torch.onnx\n",
    "torch.onnx.export(model,               # model being run\n",
    "                  trial,                  # model input (or a tuple for multiple inputs)\n",
    "                  \"transformer-imdb.onnx\", opset_version = 11)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 224.689429,
   "end_time": "2021-11-26T12:07:46.068263",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-11-26T12:04:01.378834",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
