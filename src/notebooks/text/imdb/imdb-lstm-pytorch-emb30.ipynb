{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c566e426",
   "metadata": {
    "papermill": {
     "duration": 0.013001,
     "end_time": "2021-09-27T06:40:46.045889",
     "exception": false,
     "start_time": "2021-09-27T06:40:46.032888",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Sentiment Analysis using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "554ae695",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:40:46.075421Z",
     "iopub.status.busy": "2021-09-27T06:40:46.074277Z",
     "iopub.status.idle": "2021-09-27T06:40:54.197637Z",
     "shell.execute_reply": "2021-09-27T06:40:54.196977Z",
     "shell.execute_reply.started": "2021-09-27T06:27:16.305273Z"
    },
    "papermill": {
     "duration": 8.139491,
     "end_time": "2021-09-27T06:40:54.197807",
     "exception": false,
     "start_time": "2021-09-27T06:40:46.058316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bz2\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e8ef052",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:40:54.226734Z",
     "iopub.status.busy": "2021-09-27T06:40:54.226114Z",
     "iopub.status.idle": "2021-09-27T06:40:54.228877Z",
     "shell.execute_reply": "2021-09-27T06:40:54.228436Z",
     "shell.execute_reply.started": "2021-09-27T06:27:16.333073Z"
    },
    "papermill": {
     "duration": 0.018829,
     "end_time": "2021-09-27T06:40:54.229016",
     "exception": false,
     "start_time": "2021-09-27T06:40:54.210187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bbc0fd",
   "metadata": {
    "papermill": {
     "duration": 0.012064,
     "end_time": "2021-09-27T06:40:54.253298",
     "exception": false,
     "start_time": "2021-09-27T06:40:54.241234",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1) Load in and visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf79402f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:40:54.282707Z",
     "iopub.status.busy": "2021-09-27T06:40:54.282087Z",
     "iopub.status.idle": "2021-09-27T06:40:56.620251Z",
     "shell.execute_reply": "2021-09-27T06:40:56.620746Z",
     "shell.execute_reply.started": "2021-09-27T06:27:16.350631Z"
    },
    "papermill": {
     "duration": 2.355496,
     "end_time": "2021-09-27T06:40:56.620930",
     "exception": false,
     "start_time": "2021-09-27T06:40:54.265434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21648d09",
   "metadata": {
    "papermill": {
     "duration": 0.012616,
     "end_time": "2021-09-27T06:40:56.647349",
     "exception": false,
     "start_time": "2021-09-27T06:40:56.634733",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2) Data Processing â€” convert to lower case, Remove punctuation etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1c9aada",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:40:56.701760Z",
     "iopub.status.busy": "2021-09-27T06:40:56.686117Z",
     "iopub.status.idle": "2021-09-27T06:41:04.179408Z",
     "shell.execute_reply": "2021-09-27T06:41:04.179841Z",
     "shell.execute_reply.started": "2021-09-27T06:27:17.026836Z"
    },
    "papermill": {
     "duration": 7.519946,
     "end_time": "2021-09-27T06:41:04.180010",
     "exception": false,
     "start_time": "2021-09-27T06:40:56.660064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>one reviewers mentioned watching 1 oz episode ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>basically theres family little boy jake thinks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>petter matteis love time money visually stunni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                     cleaned_reviews  \n",
       "0  one reviewers mentioned watching 1 oz episode ...  \n",
       "1  wonderful little production filming technique ...  \n",
       "2  thought wonderful way spend time hot summer we...  \n",
       "3  basically theres family little boy jake thinks...  \n",
       "4  petter matteis love time money visually stunni...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data_preprocessing(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('<.*?>', '', text) # Remove HTML from text\n",
    "    text = ''.join([c for c in text if c not in string.punctuation])# Remove punctuation\n",
    "    text = [word for word in text.split() if word not in stop_words]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "df['cleaned_reviews'] = df['review'].apply(data_preprocessing)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5959166e",
   "metadata": {
    "papermill": {
     "duration": 0.012414,
     "end_time": "2021-09-27T06:41:04.205145",
     "exception": false,
     "start_time": "2021-09-27T06:41:04.192731",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5) Tokenize â€” Create Vocab to Int mapping dictionary\n",
    "In most of the NLP tasks, you will create an index mapping dictionary in such a way that your frequently occurring words are assigned lower indexes. One of the most common way of doing this is to use Counter method from Collections library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4e4d340",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:41:04.248581Z",
     "iopub.status.busy": "2021-09-27T06:41:04.243481Z",
     "iopub.status.idle": "2021-09-27T06:41:10.707946Z",
     "shell.execute_reply": "2021-09-27T06:41:10.707094Z",
     "shell.execute_reply.started": "2021-09-27T06:27:27.947573Z"
    },
    "papermill": {
     "duration": 6.490556,
     "end_time": "2021-09-27T06:41:10.708260",
     "exception": false,
     "start_time": "2021-09-27T06:41:04.217704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size : 222610\n"
     ]
    }
   ],
   "source": [
    "max_features = 8192\n",
    "maxlen = 30\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(df['cleaned_reviews'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocabulary Size :\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f56aa037",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:41:10.781190Z",
     "iopub.status.busy": "2021-09-27T06:41:10.775766Z",
     "iopub.status.idle": "2021-09-27T06:41:14.856427Z",
     "shell.execute_reply": "2021-09-27T06:41:14.855908Z",
     "shell.execute_reply.started": "2021-09-27T06:27:35.341648Z"
    },
    "papermill": {
     "duration": 4.134053,
     "end_time": "2021-09-27T06:41:14.856571",
     "exception": false,
     "start_time": "2021-09-27T06:41:10.722518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_token = tokenizer.texts_to_sequences(df['cleaned_reviews'])\n",
    "x_data = pad_sequences(training_token, maxlen = maxlen, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42ba5fdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:41:14.888234Z",
     "iopub.status.busy": "2021-09-27T06:41:14.887575Z",
     "iopub.status.idle": "2021-09-27T06:41:14.908886Z",
     "shell.execute_reply": "2021-09-27T06:41:14.908382Z",
     "shell.execute_reply.started": "2021-09-27T06:27:41.190485Z"
    },
    "papermill": {
     "duration": 0.038822,
     "end_time": "2021-09-27T06:41:14.909034",
     "exception": false,
     "start_time": "2021-09-27T06:41:14.870212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_data = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46d64cd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:41:14.941080Z",
     "iopub.status.busy": "2021-09-27T06:41:14.939955Z",
     "iopub.status.idle": "2021-09-27T06:41:14.954386Z",
     "shell.execute_reply": "2021-09-27T06:41:14.954830Z",
     "shell.execute_reply.started": "2021-09-27T06:27:41.232067Z"
    },
    "papermill": {
     "duration": 0.032852,
     "end_time": "2021-09-27T06:41:14.955010",
     "exception": false,
     "start_time": "2021-09-27T06:41:14.922158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_remain, y_train, y_remain = train_test_split(x_data, y_data, test_size=0.2, random_state=1)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_remain, y_remain, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b500e877",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:41:14.987767Z",
     "iopub.status.busy": "2021-09-27T06:41:14.987135Z",
     "iopub.status.idle": "2021-09-27T06:41:15.006803Z",
     "shell.execute_reply": "2021-09-27T06:41:15.007306Z",
     "shell.execute_reply.started": "2021-09-27T06:27:41.252887Z"
    },
    "papermill": {
     "duration": 0.039333,
     "end_time": "2021-09-27T06:41:15.007486",
     "exception": false,
     "start_time": "2021-09-27T06:41:14.968153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create tensor dataset\n",
    "train_data = TensorDataset(torch.from_numpy(X_train.astype('float64')), torch.from_numpy(np.array(y_train).astype('float64')))\n",
    "test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(np.array(y_test).astype('float64')))\n",
    "valid_data = TensorDataset(torch.from_numpy(X_valid), torch.from_numpy(np.array(y_valid).astype('float64')))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a033086b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:41:15.043761Z",
     "iopub.status.busy": "2021-09-27T06:41:15.043136Z",
     "iopub.status.idle": "2021-09-27T06:41:15.136573Z",
     "shell.execute_reply": "2021-09-27T06:41:15.136976Z",
     "shell.execute_reply.started": "2021-09-27T06:27:41.266351Z"
    },
    "papermill": {
     "duration": 0.11658,
     "end_time": "2021-09-27T06:41:15.137163",
     "exception": false,
     "start_time": "2021-09-27T06:41:15.020583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 30])\n",
      "Sample input: \n",
      " tensor([[9.0000e+00, 1.0060e+03, 3.3700e+02,  ..., 1.8300e+02, 8.1400e+02,\n",
      "         2.1630e+03],\n",
      "        [3.4000e+01, 1.3460e+03, 4.9400e+02,  ..., 1.9750e+03, 4.4400e+02,\n",
      "         4.2000e+02],\n",
      "        [6.2560e+03, 2.6670e+03, 9.3600e+02,  ..., 1.2860e+03, 1.9300e+02,\n",
      "         2.4100e+02],\n",
      "        ...,\n",
      "        [3.7500e+02, 5.5300e+02, 1.4710e+03,  ..., 1.8480e+03, 1.9300e+02,\n",
      "         1.9300e+02],\n",
      "        [6.0700e+02, 2.4000e+01, 6.2200e+02,  ..., 2.8770e+03, 7.2000e+01,\n",
      "         2.4400e+02],\n",
      "        [6.8000e+01, 1.5210e+03, 2.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]], dtype=torch.float64)\n",
      "Sample input: \n",
      " tensor([1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0.],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print('Sample input: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cb90201",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:41:15.174590Z",
     "iopub.status.busy": "2021-09-27T06:41:15.173927Z",
     "iopub.status.idle": "2021-09-27T06:41:15.176739Z",
     "shell.execute_reply": "2021-09-27T06:41:15.177153Z",
     "shell.execute_reply.started": "2021-09-27T06:27:41.28962Z"
    },
    "papermill": {
     "duration": 0.026863,
     "end_time": "2021-09-27T06:41:15.177368",
     "exception": false,
     "start_time": "2021-09-27T06:41:15.150505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(SentimentNet, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        #self.conv = nn.Conv1d(embedding_dim, 8, kernel_size=5)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        #self.gru = nn.GRU(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        if len(x) != 1:\n",
    "            batch_size = x.size(0)\n",
    "        else:\n",
    "            batch_size = 1\n",
    "        \n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "\n",
    "        #conv = self.conv(x)\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        #hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                     weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "989fb608",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:41:15.210204Z",
     "iopub.status.busy": "2021-09-27T06:41:15.209571Z",
     "iopub.status.idle": "2021-09-27T06:41:15.212383Z",
     "shell.execute_reply": "2021-09-27T06:41:15.212807Z",
     "shell.execute_reply.started": "2021-09-27T06:27:41.304138Z"
    },
    "papermill": {
     "duration": 0.021779,
     "end_time": "2021-09-27T06:41:15.212979",
     "exception": false,
     "start_time": "2021-09-27T06:41:15.191200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "773b8715",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:41:15.245532Z",
     "iopub.status.busy": "2021-09-27T06:41:15.244842Z",
     "iopub.status.idle": "2021-09-27T06:41:15.344891Z",
     "shell.execute_reply": "2021-09-27T06:41:15.344333Z",
     "shell.execute_reply.started": "2021-09-27T06:27:41.32013Z"
    },
    "papermill": {
     "duration": 0.118471,
     "end_time": "2021-09-27T06:41:15.345019",
     "exception": false,
     "start_time": "2021-09-27T06:41:15.226548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6678881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "output_size = 1\n",
    "embedding_dim = 30\n",
    "hidden_dim = 4\n",
    "n_layers = 1\n",
    "\n",
    "model = SentimentNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "print(model_params(model))\n",
    "model.to(device)\n",
    "lr=0.008\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83e294b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:41:15.382997Z",
     "iopub.status.busy": "2021-09-27T06:41:15.382387Z",
     "iopub.status.idle": "2021-09-27T06:46:14.833315Z",
     "shell.execute_reply": "2021-09-27T06:46:14.833828Z",
     "shell.execute_reply.started": "2021-09-27T06:27:41.364585Z"
    },
    "papermill": {
     "duration": 299.47532,
     "end_time": "2021-09-27T06:46:14.834202",
     "exception": false,
     "start_time": "2021-09-27T06:41:15.358882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5... Step: 100... Loss: 0.690872... Val Loss: 0.679388\n",
      "Validation loss decreased (inf --> 0.679388).  Saving model ...\n",
      "Epoch: 1/5... Step: 200... Loss: 0.589363... Val Loss: 0.643219\n",
      "Validation loss decreased (0.679388 --> 0.643219).  Saving model ...\n",
      "Epoch: 1/5... Step: 300... Loss: 0.651924... Val Loss: 0.571832\n",
      "Validation loss decreased (0.643219 --> 0.571832).  Saving model ...\n",
      "Epoch: 1/5... Step: 400... Loss: 0.575346... Val Loss: 0.560953\n",
      "Validation loss decreased (0.571832 --> 0.560953).  Saving model ...\n",
      "Epoch: 1/5... Step: 500... Loss: 0.582748... Val Loss: 0.527904\n",
      "Validation loss decreased (0.560953 --> 0.527904).  Saving model ...\n",
      "Epoch: 1/5... Step: 600... Loss: 0.643364... Val Loss: 0.505855\n",
      "Validation loss decreased (0.527904 --> 0.505855).  Saving model ...\n",
      "Epoch: 1/5... Step: 700... Loss: 0.611294... Val Loss: 0.496798\n",
      "Validation loss decreased (0.505855 --> 0.496798).  Saving model ...\n",
      "Epoch: 1/5... Step: 800... Loss: 0.453555... Val Loss: 0.487044\n",
      "Validation loss decreased (0.496798 --> 0.487044).  Saving model ...\n",
      "Epoch: 2/5... Step: 900... Loss: 0.474545... Val Loss: 0.472480\n",
      "Validation loss decreased (0.487044 --> 0.472480).  Saving model ...\n",
      "Epoch: 2/5... Step: 1000... Loss: 0.579688... Val Loss: 0.473078\n",
      "Epoch: 2/5... Step: 1100... Loss: 0.623941... Val Loss: 0.459840\n",
      "Validation loss decreased (0.472480 --> 0.459840).  Saving model ...\n",
      "Epoch: 2/5... Step: 1200... Loss: 0.467315... Val Loss: 0.457918\n",
      "Validation loss decreased (0.459840 --> 0.457918).  Saving model ...\n",
      "Epoch: 2/5... Step: 1300... Loss: 0.524194... Val Loss: 0.450999\n",
      "Validation loss decreased (0.457918 --> 0.450999).  Saving model ...\n",
      "Epoch: 2/5... Step: 1400... Loss: 0.548538... Val Loss: 0.461955\n",
      "Epoch: 2/5... Step: 1500... Loss: 0.494353... Val Loss: 0.451950\n",
      "Epoch: 2/5... Step: 1600... Loss: 0.439279... Val Loss: 0.450572\n",
      "Validation loss decreased (0.450999 --> 0.450572).  Saving model ...\n",
      "Epoch: 3/5... Step: 1700... Loss: 0.317050... Val Loss: 0.444229\n",
      "Validation loss decreased (0.450572 --> 0.444229).  Saving model ...\n",
      "Epoch: 3/5... Step: 1800... Loss: 0.484679... Val Loss: 0.450134\n",
      "Epoch: 3/5... Step: 1900... Loss: 0.433740... Val Loss: 0.442165\n",
      "Validation loss decreased (0.444229 --> 0.442165).  Saving model ...\n",
      "Epoch: 3/5... Step: 2000... Loss: 0.485951... Val Loss: 0.446052\n",
      "Epoch: 3/5... Step: 2100... Loss: 0.436717... Val Loss: 0.447202\n",
      "Epoch: 3/5... Step: 2200... Loss: 0.525011... Val Loss: 0.445001\n",
      "Epoch: 3/5... Step: 2300... Loss: 0.452243... Val Loss: 0.438154\n",
      "Validation loss decreased (0.442165 --> 0.438154).  Saving model ...\n",
      "Epoch: 3/5... Step: 2400... Loss: 0.372045... Val Loss: 0.439130\n",
      "Epoch: 4/5... Step: 2500... Loss: 0.346176... Val Loss: 0.452154\n",
      "Epoch: 4/5... Step: 2600... Loss: 0.395973... Val Loss: 0.438350\n",
      "Epoch: 4/5... Step: 2700... Loss: 0.446884... Val Loss: 0.440824\n",
      "Epoch: 4/5... Step: 2800... Loss: 0.410138... Val Loss: 0.438511\n",
      "Epoch: 4/5... Step: 2900... Loss: 0.368834... Val Loss: 0.438703\n",
      "Epoch: 4/5... Step: 3000... Loss: 0.346059... Val Loss: 0.435683\n",
      "Validation loss decreased (0.438154 --> 0.435683).  Saving model ...\n",
      "Epoch: 4/5... Step: 3100... Loss: 0.375837... Val Loss: 0.434819\n",
      "Validation loss decreased (0.435683 --> 0.434819).  Saving model ...\n",
      "Epoch: 4/5... Step: 3200... Loss: 0.380935... Val Loss: 0.440387\n",
      "Epoch: 5/5... Step: 3300... Loss: 0.464603... Val Loss: 0.451909\n",
      "Epoch: 5/5... Step: 3400... Loss: 0.337613... Val Loss: 0.446164\n",
      "Epoch: 5/5... Step: 3500... Loss: 0.319030... Val Loss: 0.433858\n",
      "Validation loss decreased (0.434819 --> 0.433858).  Saving model ...\n",
      "Epoch: 5/5... Step: 3600... Loss: 0.402803... Val Loss: 0.442518\n",
      "Epoch: 5/5... Step: 3700... Loss: 0.305911... Val Loss: 0.438783\n",
      "Epoch: 5/5... Step: 3800... Loss: 0.246179... Val Loss: 0.443382\n",
      "Epoch: 5/5... Step: 3900... Loss: 0.264731... Val Loss: 0.442106\n",
      "Epoch: 5/5... Step: 4000... Loss: 0.519473... Val Loss: 0.438593\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    h = model.init_hidden(batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        loss = criterion(output, labels.float())\n",
    "        loss.backward()\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        if counter%print_every == 0:\n",
    "            val_h = model.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in test_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp, val_h)\n",
    "                val_loss = criterion(out, lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './state_dict.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a82f9053",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:46:14.884098Z",
     "iopub.status.busy": "2021-09-27T06:46:14.883514Z",
     "iopub.status.idle": "2021-09-27T06:46:14.901300Z",
     "shell.execute_reply": "2021-09-27T06:46:14.901766Z",
     "shell.execute_reply.started": "2021-09-27T06:27:41.42034Z"
    },
    "papermill": {
     "duration": 0.044475,
     "end_time": "2021-09-27T06:46:14.901969",
     "exception": false,
     "start_time": "2021-09-27T06:46:14.857494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9342], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = model.init_hidden(1)\n",
    "sentence = \"I love this movie because it is great\"\n",
    "trial = torch.tensor(pad_sequences(tokenizer.texts_to_sequences([sentence]), maxlen = maxlen)).to(device)\n",
    "\n",
    "\n",
    "model(trial, h)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79fe44ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:46:14.953153Z",
     "iopub.status.busy": "2021-09-27T06:46:14.952535Z",
     "iopub.status.idle": "2021-09-27T06:46:15.270001Z",
     "shell.execute_reply": "2021-09-27T06:46:15.270532Z"
    },
    "papermill": {
     "duration": 0.345756,
     "end_time": "2021-09-27T06:46:15.270721",
     "exception": false,
     "start_time": "2021-09-27T06:46:14.924965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:18: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "/opt/conda/lib/python3.7/site-packages/torch/onnx/symbolic_opset9.py:1805: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  \"or define the initial states (h0/c0) as inputs of the model. \")\n"
     ]
    }
   ],
   "source": [
    "import torch.onnx\n",
    "torch.onnx.export(model,               # model being run\n",
    "                  (trial, (torch.randn(1,1,4).to(device), torch.randn(1,1,4).to(device))),                   # model input (or a tuple for multiple inputs)\n",
    "                  \"lstm.onnx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 338.939654,
   "end_time": "2021-09-27T06:46:17.702489",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-27T06:40:38.762835",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
